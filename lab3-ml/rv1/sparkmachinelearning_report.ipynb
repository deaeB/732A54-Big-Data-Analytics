{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"from __future__ import division\n",
				"from math import radians, cos, sin, asin, sqrt, exp\n",
				"from datetime import datetime\n",
				"from pyspark import SparkContext, StorageLevel\n",
				"\n",
				"sc = SparkContext(appName = \"Lab\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# sc.stop()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"def haversine(lon1, lat1, lon2, lat2):\n",
				"    \"\"\"\n",
				"    Calculate the great circle distance between two points\n",
				"    on the earth (specified in decimal degrees)\n",
				"    \"\"\"\n",
				"    # convert decimal degrees to radians\n",
				"    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
				"    # haversine formula\n",
				"    dlon = lon2 - lon1\n",
				"    dlat = lat2 - lat1\n",
				"    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
				"    c = 2 * asin(sqrt(a))\n",
				"    km = 6367 * c\n",
				"    return km\n",
				"\n",
				"def d_date(date1,date2):\n",
				"    \"\"\"\n",
				"    Calculate the distance between two day\n",
				"    \"\"\"\n",
				"    date_diff=abs((date2 - date1).days)\n",
				"    return date_diff\n",
				"\n",
				"def d_time(time1,time2):\n",
				"    \"\"\"\n",
				"    Calculate the distance between two time\n",
				"    \"\"\"\n",
				"    time_diff=abs((time2 - time1).total_seconds() / 3600)\n",
				"    return time_diff\n",
				"\n",
				"def sum_kernel(d_distance,d_date,d_time):\n",
				"    \"\"\"\n",
				"    Calculate the sum of the kernel\n",
				"    \"\"\"\n",
				"    distance_kernel=exp(-(d_distance**2)/(h_distance**2))\n",
				"    time_kernel=exp(-(d_time**2)/(h_time**2))\n",
				"    date_kernel=exp(-(d_date**2)/(h_date**2))\n",
				"    \n",
				"    return distance_kernel+time_kernel+date_kernel\n",
				"\n",
				"def prod_kernel(d_distance,d_date,d_time):\n",
				"    \"\"\"\n",
				"    Calculate the prod of the kernel\n",
				"    \"\"\"\n",
				"    distance_kernel=exp(-(d_distance**2)/(h_distance**2))\n",
				"    time_kernel=exp(-(d_time**2)/(h_time**2))\n",
				"    date_kernel=exp(-(d_date**2)/(h_date**2))\n",
				"    \n",
				"    return distance_kernel*time_kernel*date_kernel\n",
				"    \n",
				"\n",
				"h_distance = 480\n",
				"h_date = 6\n",
				"h_time = 3\n",
				"a = 58.4274 # Up to you\n",
				"b = 14.826 # Up to you\n",
				"date = \"2014-11-04\" # Up to you"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"PythonRDD[5] at RDD at PythonRDD.scala:53\n"
					]
				}
			],
			"source": [
				"DATE_TIMESTAMP=datetime.strptime(date,\"%Y-%m-%d\")\n",
				"\n",
				"stations = sc.textFile(\"s3://bigdatalabs7051/data/stations/stations.csv\")\n",
				"temps = sc.textFile(\"s3://bigdatalabs7051/data/temperature-readings/temperature-readings.csv\")\n",
				"# lines = temps.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),datetime.strptime(x[1],\"%Y-%m-%d\"),datetime.strptime(x[2],\"%H:%M:%S\"),float(x[3])))\n",
				"\n",
				"# Broadcast\n",
				"# Station data: station_num, distance\n",
				"stations_data=stations.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),haversine(b,a,float(x[4]),float(x[3]))))\n",
				"bc=sc.broadcast(stations_data.collectAsMap())    # Convert the rdd stations_data to dictionary, then broadcast it.\n",
				"\n",
				"# Join\n",
				"# intermediate: station_num,(date, time, temperature)\n",
				"# joined_data: station_num, (geo distance, date distance, time, temperature)\n",
				"joined_data = temps.sample(False, 0.1).map(lambda line: line.split(\";\"))\\\n",
				"                .map(lambda x:(str(x[0]),(datetime.strptime(x[1],\"%Y-%m-%d\"),datetime.strptime(x[2],\"%H:%M:%S\"),float(x[3]))))\\\n",
				"                .filter(lambda x: (x[1][0]<=DATE_TIMESTAMP))\\\n",
				"                .map(lambda x: (x[0],(bc.value[x[0]],d_date(x[1][0],DATE_TIMESTAMP),x[1][1],x[1][2])))\n",
				"# date filtered here, time will be filted when looping over times\n",
				"\n",
				"# Persistence\n",
				"joined_data.cache()\n",
				"\n",
				"# print(joined_data.collect())\n"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Task 1: Use a kernel that is the sum of three Gaussian kernels"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 26,
			"metadata": {
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"y_sum=[]\n",
				"\n",
				"for time in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n",
				"    # Your code here\n",
				"    \n",
				"    # sum_kernel_rdd: 1, (sum_kernel, temperature)\n",
				"    #sum_kernel_rdd=joined_data.map(lambda x:(1,(sum_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3])))\n",
				"    #sum_kernel_rdd.cache()\n",
				"    #denominator=sum_kernel_rdd.groupByKey().mapValues(lambda values: sum([v[0] for v in values]))\n",
				"    #numerator=sum_kernel_rdd.groupByKey().mapValues(lambda values: sum([v[0]*v[1] for v in values]))\n",
				"    \n",
				"    # Avoid using groupByKey and mapValues because it's much slower than reduceByKey\n",
				"\n",
				"    # sum_kernel_rdd: 1, (sum_kernel, temperature)\n",
				"    time=datetime.strptime(time,\"%H:%M:%S\")\n",
				"    sum_kernel_rdd=joined_data.filter(lambda x:(x[1][1]>0 or (x[1][2] == 0 and  x[1][2]<time)))\\\n",
				"        .map(lambda x:(sum_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3]))\n",
				"    sum_kernel_rdd=sum_kernel_rdd.map(lambda x:(x[0],x[0]*x[1])).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
				"    #The result type returned by the reduce operation is consistent with the element type in the RDD. What is returned here is a tuple\n",
				"    y_sum.append(sum_kernel_rdd[1]/sum_kernel_rdd[0])\n",
				"    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"time_list=[\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
				"dictionary = {k: v for k, v in zip(time_list, y_sum)}\n",
				"print(\"latitude,longitude:\",a,\",\",b)\n",
				"print(\"date:\",date)\n",
				"print(dictionary)\n",
				"print(\"HyperParams:\")\n",
				"print(\"h_distance:\",h_distance)\n",
				"print(\"h_date:\",h_date)\n",
				"print(\"h_time:\",h_time)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"output:  \n",
				"latitude,longitude: 58.4274 , 14.826  \n",
				"date: 2014-11-04  \n",
				"{'0:00:00': 5.570851580782074, '22:00:00': 5.741075353218582, '20:00:00': 5.729431354715042, '18:00:00': 5.8618844900509455, '16:00:00': 6.076963659810907, '14:00:00': 6.247533933860337, '12:00:00': 6.249859464055124, '10:00:00': 6.048901637752291, '08:00:00': 5.679243570182346, '06:00:00': 5.359907996425515, '04:00:00': 5.2696688195185315}  \n",
				"HyperParams:  \n",
				"h_distance: 480  \n",
				"h_date: 6  \n",
				"h_time: 3  \n"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Q1:** Show that your choice for the kernelsâ€™ width is sensible, i.e. it gives more weight to closer points. Discuss why your definition of closeness is reasonable.\n",
				"\n",
				"**Answer:** The parameter of the Gaussian kernel function is the bandwidth, which determines the decay speed and smoothness of the kernel function. The smaller the bandwidth, the narrower the scope of the kernel function, the greater the impact of the current parameter change (in other words: more consideration of the current parameter), and vice versa.\n",
				"\n",
				"According to common sense, time has the greatest impact on temperature, and date also has a certain impact on temperature. For example, winter is colder and summer is hotter, and the impact of distance is the least obvious (the distance from Oslo to Copenhagen is 563km, but the temperature difference is sometimes less than 1 degree Celsius). . Therefore the choice for time bandwidth should be the smallest and for the distance bandwidth the choice should be the largest.\n",
				"\n",
				"We choose h_distance=483 because the distance from Uppsala to Tampere is 483km, and the average temperature difference between the two places on May 18 is 5 degrees Celsius.\n",
				"\n",
				"Choose h_date=6, because Linkoping's average daily temperature increased by 5 degrees Celsius from May 18 to May 24.\n",
				"\n",
				"Choose h_time=3, because Linkoping May 18 from 7:00 to 10:00, the temperature increased by 5 degrees Celsius"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Task 2: Use a kernel that is the prod of three Gaussian kernels"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 28,
			"metadata": {
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"y_prod=[]\n",
				"\n",
				"for time in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n",
				"    # Your code here\n",
				"    # prod_kernel_rdd: 1, (prod_kernel, temperature)\n",
				"    time=datetime.strptime(time,\"%H:%M:%S\")\n",
				"    prod_kernel_rdd=joined_data.filter(lambda x:(x[1][1]>0 or (x[1][2] == 0 and  x[1][2]<time)))\\\n",
				"        .map(lambda x:(prod_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3]))\n",
				"    prod_kernel_rdd=prod_kernel_rdd.map(lambda x:(x[0],x[0]*x[1])).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
				"    #The result type returned by the reduce operation is consistent with the element type in the RDD. What is returned here is a tuple\n",
				"    y_prod.append(prod_kernel_rdd[1]/prod_kernel_rdd[0])\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"time_list=[\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
				"dictionary = {k: v for k, v in zip(time_list, y_prod)}\n",
				"print(\"latitude,longitude:\",a,\",\",b)\n",
				"print(\"date:\",date)\n",
				"print(dictionary)\n",
				"print(\"HyperParams:\")\n",
				"print(\"h_distance:\",h_distance)\n",
				"print(\"h_date:\",h_date)\n",
				"print(\"h_time:\",h_time)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"output:  \n",
				"latitude,longitude: 58.4274 , 14.826  \n",
				"date: 2014-11-04  \n",
				"{'0:00:00': 7.303006795891909, '22:00:00': 7.982068570228097, '20:00:00': 8.12157139414748, '18:00:00': 8.22400511629453, '16:00:00': 8.40477703134592, '14:00:00': 8.868385461565387, '12:00:00': 9.113797702887448, '10:00:00': 8.690957561793367, '08:00:00': 7.987008400145532, '06:00:00': 7.51869787901216, '04:00:00': 7.335325908719835}  \n",
				"HyperParams:  \n",
				"h_distance: 480  \n",
				"h_date: 6  \n",
				"h_time: 3  "
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Q2:** Repeat the exercise using a kernel that is the product of the three Gaussian kernels above. Compare the results with those obtained for the additive kernel. If they differ, explain why.\n",
				"\n",
				"**Answer:** Using the product of the three Gaussian kernels, overall, the predicted air temperature is higher than using the kernel that is the sum of three Gaussian kernels.\n",
				"\n",
				"The product of Gaussian kernel functions can enhance the similarity between samples, that is, when the values of the three Gaussian kernel functions are larger at the same time, the similarity is higher. Conversely, if one of the Gaussian kernels has a lower value, then the kernel value will also be lower. Therefore, it pays more attention to the common characteristics of the three samples in the feature space.\n",
				"The sum of Gaussian kernel functions can comprehensively consider the similarity between samples. It pays more attention to the respective characteristics of the three samples in the feature space, and considers them comprehensively by adding."
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Task 3: Using at least two MLlib library models to predict the hourly temperatures for a date and place in Sweden."
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Linear Regression"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from datetime import date\n",
				"\n",
				"# Transform time to float,range from 0-100\n",
				"def time_to_float(time):\n",
				"    hour, minute, sec= map(int, time.split(\":\"))\n",
				"    total_minutes = hour * 60 + minute\n",
				"    relative_minutes = total_minutes / (24 * 60)\n",
				"    return relative_minutes * 100\n",
				"\n",
				"# Transform date to float,range from 0-100\n",
				"def date_to_float(date_str):\n",
				"    year, month, day = map(int, date_str.split(\"-\"))\n",
				"    date_obj = date(year, month, day)\n",
				"    year_start = date(year, 1, 1)\n",
				"    year_end = date(year, 12, 31)\n",
				"    total_days = (date_obj - year_start).days\n",
				"    relative_days = total_days / (year_end - year_start).days\n",
				"    return relative_days * 100"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"\n",
				"from pyspark.mllib.regression import LabeledPoint\n",
				"from pyspark.mllib.regression import LinearRegressionWithSGD\n",
				"\n",
				"# Broadcast\n",
				"# station_num, longitude, latitude\n",
				"stations_data=stations.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),(float(x[4]),float(x[3]))))\n",
				"bc=sc.broadcast(stations_data.collectAsMap())    # Convert the rdd stations_data to dictionary, then broadcast it.\n",
				"\n",
				"\n",
				"\n",
				"\n",
				"# Predict!\n",
				"a = 58.4274 # latitude\n",
				"b = 14.826 # longitude\n",
				"date1 = \"2014-11-04\" # Up to you\n",
				"\n",
				"y_linreg=[]\n",
				"predict_data=[]\n",
				"for time1 in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n",
				"    # intermediate: (longitude, latitude),day,time,temp\n",
				"    # LabeledPoint(labeled_data) : temperature, longitude, latitude, date, time\n",
				"    labeled_data = temps.sample(False, 0.1).map(lambda line: line.split(\";\"))\\\n",
				"                    .map(lambda x:(bc.value[str(x[0])],date_to_float(x[1]),time_to_float(x[2]),float(x[3])))\\\n",
				"                    .filter(lambda x: x[1]<date_to_float(date1) or (x[1]==date_to_float(date1) and x[2]<time_to_float(time1)) )\\\n",
				"                    .map(lambda x: LabeledPoint(x[3], [x[0][0], x[0][1], x[1], x[2]]))\n",
				"                    \n",
				"\n",
				"    train_data, test_data = labeled_data.randomSplit([0.7, 0.3])\n",
				"\n",
				"    # Train the linear regression model\n",
				"    model = LinearRegressionWithSGD.train(train_data, iterations=100, step=0.0005)\n",
				"\n",
				"\n",
				"    # Your code here\n",
				"    predict_data.append([b,a,date_to_float(date1),time_to_float(time1)])\n",
				"    \n",
				"rdd1 = sc.parallelize(predict_data)\n",
				"print(\"Training data:\")\n",
				"print(rdd1.collect())\n",
				"predictions = model.predict(rdd1)\n",
				"y_linreg=predictions.collect()\n",
				"\n",
				"print(\"Predict result:\")\n",
				"print(y_linreg)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Training data:  \n",
				"[[14.826, 58.4274, 84.34065934065934, 0.0], [14.826, 58.4274, 84.34065934065934, 91.66666666666666], [14.826, 58.4274, 84.34065934065934, 83.33333333333334], [14.826, 58.4274, 84.34065934065934, 75.0], [14.826, 58.4274, 84.34065934065934, 66.66666666666666], [14.826, 58.4274, 84.34065934065934, 58.333333333333336], [14.826, 58.4274, 84.34065934065934, 50.0], [14.826, 58.4274, 84.34065934065934, 41.66666666666667], [14.826, 58.4274, 84.34065934065934, 33.33333333333333], [14.826, 58.4274, 84.34065934065934, 25.0], [14.826, 58.4274, 84.34065934065934, 16.666666666666664]]  \n",
				"Predict result:  \n",
				"[13.604115554952378, 15.996997551067711, 15.779462824148135, 15.56192809722856, 15.344393370308984, 15.126858643389408, 14.909323916469832, 14.691789189550256, 14.47425446263068, 14.256719735711105, 14.039185008791529]  "
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### ridge regression"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"from pyspark.mllib.regression import LabeledPoint\n",
				"from pyspark.mllib.regression import RidgeRegressionWithSGD\n",
				"\n",
				"# Broadcast\n",
				"# station_num, longitude, latitude\n",
				"stations_data=stations.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),(float(x[4]),float(x[3]))))\n",
				"bc=sc.broadcast(stations_data.collectAsMap())    # Convert the rdd stations_data to dictionary, then broadcast it.\n",
				"\n",
				"\n",
				"\n",
				"\n",
				"# Predict!\n",
				"a = 58.4274 # latitude\n",
				"b = 14.826 # longitude\n",
				"date1 = \"2014-11-04\" # Up to you\n",
				"\n",
				"y_linreg=[]\n",
				"predict_data=[]\n",
				"for time1 in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
				"\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n",
				"    # intermediate: (longitude, latitude),day,time,temp\n",
				"    # LabeledPoint(labeled_data) : temperature, longitude, latitude, date, time\n",
				"    labeled_data = temps.sample(False, 0.1).map(lambda line: line.split(\";\"))\\\n",
				"                    .map(lambda x:(bc.value[str(x[0])],date_to_float(x[1]),time_to_float(x[2]),float(x[3])))\\\n",
				"                    .filter(lambda x: x[1]<date_to_float(date1) or (x[1]==date_to_float(date1) and x[2]<time_to_float(time1)) )\\\n",
				"                    .map(lambda x: LabeledPoint(x[3], [x[0][0], x[0][1], x[1], x[2]]))\n",
				"                    \n",
				"\n",
				"    train_data, test_data = labeled_data.randomSplit([0.7, 0.3])\n",
				"\n",
				"    # Train the linear regression model\n",
				"    model = RidgeRegressionWithSGD.train(train_data, iterations=100, step=0.0005)\n",
				"\n",
				"\n",
				"    # Your code here\n",
				"    predict_data.append([b,a,date_to_float(date1),time_to_float(time1)])\n",
				"    \n",
				"rdd1 = sc.parallelize(predict_data)\n",
				"print(\"Training data:\")\n",
				"print(rdd1.collect())\n",
				"predictions = model.predict(rdd1)\n",
				"y_linreg=predictions.collect()\n",
				"\n",
				"print(\"Predict result:\")\n",
				"print(y_linreg)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Training data:  \n",
				"[[14.826, 58.4274, 84.34065934065934, 0.0], [14.826, 58.4274, 84.34065934065934, 91.66666666666666], [14.826, 58.4274, 84.34065934065934, 83.33333333333334], [14.826, 58.4274, 84.34065934065934, 75.0], [14.826, 58.4274, 84.34065934065934, 66.66666666666666], [14.826, 58.4274, 84.34065934065934, 58.333333333333336], [14.826, 58.4274, 84.34065934065934, 50.0], [14.826, 58.4274, 84.34065934065934, 41.66666666666667], [14.826, 58.4274, 84.34065934065934, 33.33333333333333], [14.826, 58.4274, 84.34065934065934, 25.0], [14.826, 58.4274, 84.34065934065934, 16.666666666666664]]  \n",
				"Predict result:  \n",
				"[13.57574968051529, 15.985517198023985, 15.766447423705014, 15.547377649386041, 15.328307875067068, 15.109238100748097, 14.890168326429125, 14.671098552110152, 14.45202877779118, 14.232959003472207, 14.013889229153236]  "
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Q3:**\n",
				"Repeat the exercise using at least two MLlib library models to predict the hourly temperatures for a date and place in Sweden. Compare the results with two Gaussian\n",
				"kernels. If they differ, explain why  \n",
				"\n",
				"**Answer:**\n",
				"we can see that the predicted result from LinearRegressionWithSGD and RidgeRegressionWithSGD are all similar to each other(most critical hyperparameters are set to the same, there isn't too much features and the regularizer parameter is set as 0.01 by default which didn't make too significant impact). And the regression results are not to kernel results. It might be caused by the hyperparameters(such as learning rate) are still needs to be optimized to get a better prediction."
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "base",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.9.13"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
