{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0 and 3.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom __future__ import division\nfrom math import radians, cos, sin, asin, sqrt, exp\nfrom datetime import datetime\nfrom pyspark import SparkContext, StorageLevel\n\nsc = SparkContext(appName = \"Lab\")",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# sc.stop()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a))\n    km = 6367 * c\n    return km\n\ndef d_date(date1,date2):\n    \"\"\"\n    Calculate the distance between two day\n    \"\"\"\n    date_diff=abs((date2 - date1).days)\n    return date_diff\n\ndef d_time(time1,time2):\n    \"\"\"\n    Calculate the distance between two time\n    \"\"\"\n    time_diff=abs((time2 - time1).total_seconds() / 3600)\n    return time_diff\n\ndef sum_kernel(d_distance,d_date,d_time):\n    \"\"\"\n    Calculate the sum of the kernel\n    \"\"\"\n    distance_kernel=exp(-(d_distance**2)/(h_distance**2))\n    time_kernel=exp(-(d_time**2)/(h_time**2))\n    date_kernel=exp(-(d_date**2)/(h_date**2))\n    \n    return distance_kernel+time_kernel+date_kernel\n\ndef prod_kernel(d_distance,d_date,d_time):\n    \"\"\"\n    Calculate the prod of the kernel\n    \"\"\"\n    distance_kernel=exp(-(d_distance**2)/(h_distance**2))\n    time_kernel=exp(-(d_time**2)/(h_time**2))\n    date_kernel=exp(-(d_date**2)/(h_date**2))\n    \n    return distance_kernel*time_kernel*date_kernel\n    \n\nh_distance = 480\nh_date = 6\nh_time = 3\na = 58.4274 # Up to you\nb = 14.826 # Up to you\ndate = \"2014-11-04\" # Up to you",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "DATE_TIMESTAMP=datetime.strptime(date,\"%Y-%m-%d\")\nstations = sc.textFile(\"s3://bigdatalabs7051/data/stations/stations.csv\")\ntemps = sc.textFile(\"s3://bigdatalabs7051/data/temperature-readings/temperature-readings.csv\")\n# lines = temps.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),datetime.strptime(x[1],\"%Y-%m-%d\"),datetime.strptime(x[2],\"%H:%M:%S\"),float(x[3])))\n\n# Broadcast\n# Station data: station_num, distance\nstations_data=stations.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),haversine(b,a,float(x[4]),float(x[3]))))\nbc=sc.broadcast(stations_data.collectAsMap())    # Convert the rdd stations_data to dictionary, then broadcast it.\n\n# Join\n# joined_data: station_num, (geo distance, date distance, time, temperature)\njoined_data = temps.sample(False, 0.1).map(lambda line: line.split(\";\"))\\\n                .map(lambda x:(str(x[0]),(datetime.strptime(x[1],\"%Y-%m-%d\"),datetime.strptime(x[2],\"%H:%M:%S\"),float(x[3]))))\\\n                .filter(lambda x: (x[1][0]<DATE_TIMESTAMP))\\\n                .map(lambda x: (x[0],(bc.value[x[0]],d_date(x[1][0],DATE_TIMESTAMP),x[1][1],x[1][2])))\n\n# Persistence\njoined_data.cache()\n\n# print(joined_data.collect())\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "PythonRDD[5] at RDD at PythonRDD.scala:53\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Task 1: Use a kernel that is the sum of three Gaussian kernels",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "y_sum=[]\n\nfor time in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n    # Your code here\n    \n    # sum_kernel_rdd: 1, (sum_kernel, temperature)\n    #sum_kernel_rdd=joined_data.map(lambda x:(1,(sum_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3])))\n    #sum_kernel_rdd.cache()\n    #denominator=sum_kernel_rdd.groupByKey().mapValues(lambda values: sum([v[0] for v in values]))\n    #numerator=sum_kernel_rdd.groupByKey().mapValues(lambda values: sum([v[0]*v[1] for v in values]))\n    \n    # Avoid using groupByKey and mapValues because it's much slower than reduceByKey\n\n    # sum_kernel_rdd: 1, (sum_kernel, temperature)\n    time=datetime.strptime(time,\"%H:%M:%S\")\n    sum_kernel_rdd=joined_data.map(lambda x:(sum_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3]))\n    sum_kernel_rdd=sum_kernel_rdd.map(lambda x:(x[0],x[0]*x[1])).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n    #The result type returned by the reduce operation is consistent with the element type in the RDD. What is returned here is a tuple\n    y_sum.append(sum_kernel_rdd[1]/sum_kernel_rdd[0])\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "time_list=[\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\ndictionary = {k: v for k, v in zip(time_list, y_sum)}\nprint(\"latitude,longitude:\",a,\",\",b)\nprint(\"date:\",date)\nprint(dictionary)\nprint(\"HyperParams:\")\nprint(\"h_distance:\",h_distance)\nprint(\"h_date:\",h_date)\nprint(\"h_time:\",h_time)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "latitude,longitude: 58.4274 , 14.826\ndate: 2014-11-04\n{'0:00:00': 5.576631957058849, '22:00:00': 5.746327086093085, '20:00:00': 5.736053132088273, '18:00:00': 5.869071519404494, '16:00:00': 6.082216782048857, '14:00:00': 6.252550240405444, '12:00:00': 6.255958093164522, '10:00:00': 6.0550847452374175, '08:00:00': 5.685894263735923, '06:00:00': 5.366830140637157, '04:00:00': 5.275485830982766}\nHyperParams:\nh_distance: 480\nh_date: 6\nh_time: 3\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "**Q1:** Show that your choice for the kernelsâ€™ width is sensible, i.e. it gives more weight to closer points. Discuss why your definition of closeness is reasonable.\n\n**Answer:** The parameter of the Gaussian kernel function is the bandwidth, which determines the decay speed and smoothness of the kernel function. The smaller the bandwidth, the narrower the scope of the kernel function, the greater the impact of the current parameter change (in other words: more consideration of the current parameter), and vice versa.\n\nAccording to common sense, time has the greatest impact on temperature, and date also has a certain impact on temperature. For example, winter is colder and summer is hotter, and the impact of distance is the least obvious (the distance from Oslo to Copenhagen is 563km, but the temperature difference is sometimes less than 1 degree Celsius). . Therefore the choice for time bandwidth should be the smallest and for the distance bandwidth the choice should be the largest.\n\nWe choose h_distance=483 because the distance from Uppsala to Tampere is 483km, and the average temperature difference between the two places on May 18 is 5 degrees Celsius.\n\nChoose h_date=6, because Linkoping's average daily temperature increased by 5 degrees Celsius from May 18 to May 24.\n\nChoose h_time=3, because Linkoping May 18 from 7:00 to 10:00, the temperature increased by 5 degrees Celsius",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Task 2: Use a kernel that is the prod of three Gaussian kernels",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "y_prod=[]\n\nfor time in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n    # Your code here\n    # prod_kernel_rdd: 1, (prod_kernel, temperature)\n    time=datetime.strptime(time,\"%H:%M:%S\")\n    prod_kernel_rdd=joined_data.map(lambda x:(prod_kernel(x[1][0],x[1][1],d_time(time,x[1][2])),x[1][3]))\n    prod_kernel_rdd=prod_kernel_rdd.map(lambda x:(x[0],x[0]*x[1])).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n    #The result type returned by the reduce operation is consistent with the element type in the RDD. What is returned here is a tuple\n    y_prod.append(prod_kernel_rdd[1]/prod_kernel_rdd[0])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "time_list=[\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\ndictionary = {k: v for k, v in zip(time_list, y_prod)}\nprint(\"latitude,longitude:\",a,\",\",b)\nprint(\"date:\",date)\nprint(dictionary)\nprint(\"HyperParams:\")\nprint(\"h_distance:\",h_distance)\nprint(\"h_date:\",h_date)\nprint(\"h_time:\",h_time)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "latitude,longitude: 58.4274 , 14.826\ndate: 2014-11-04\n{'0:00:00': 6.925568411088358, '22:00:00': 7.7684882157681265, '20:00:00': 7.920901094219781, '18:00:00': 8.067762477225351, '16:00:00': 8.378497070124379, '14:00:00': 8.823294422587335, '12:00:00': 8.914151426197472, '10:00:00': 8.446565636573004, '08:00:00': 7.774179683273953, '06:00:00': 7.265615572246946, '04:00:00': 6.994351544760919}\nHyperParams:\nh_distance: 480\nh_date: 6\nh_time: 3\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "**Q2:** Repeat the exercise using a kernel that is the product of the three Gaussian kernels above. Compare the results with those obtained for the additive kernel. If they differ, explain why.\n\n**Answer:** Using the product of the three Gaussian kernels, overall, the predicted air temperature is higher than using the kernel that is the sum of three Gaussian kernels.\n\nThe product of Gaussian kernel functions can enhance the similarity between samples, that is, when the values of the three Gaussian kernel functions are larger at the same time, the similarity is higher. Conversely, if one of the Gaussian kernels has a lower value, then the kernel value will also be lower. Therefore, it pays more attention to the common characteristics of the three samples in the feature space.\nThe sum of Gaussian kernel functions can comprehensively consider the similarity between samples. It pays more attention to the respective characteristics of the three samples in the feature space, and considers them comprehensively by adding.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Task 3: Using at least two MLlib library models to predict the hourly temperatures for a date and place in Sweden.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### Linear Regression",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from datetime import date\n\n# Transform time to float,range from 0-100\ndef time_to_float(time):\n    hour, minute, sec= map(int, time.split(\":\"))\n    total_minutes = hour * 60 + minute\n    relative_minutes = total_minutes / (24 * 60)\n    return relative_minutes * 100\n\n# Transform date to float,range from 0-100\ndef date_to_float(date_str):\n    year, month, day = map(int, date_str.split(\"-\"))\n    date_obj = date(year, month, day)\n    year_start = date(year, 1, 1)\n    year_end = date(year, 12, 31)\n    total_days = (date_obj - year_start).days\n    relative_days = total_days / (year_end - year_start).days\n    return relative_days * 100",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\n\n# Broadcast\n# station_num, longitude, latitude\nstations_data=stations.map(lambda line: line.split(\";\")).map(lambda x:(str(x[0]),(float(x[4]),float(x[3]))))\nbc=sc.broadcast(stations_data.collectAsMap())    # Convert the rdd stations_data to dictionary, then broadcast it.\n\n# LabeledPoint(labeled_data) : temperature, longitude, latitude, date, time\nlabeled_data = temps.sample(False, 0.1).map(lambda line: line.split(\";\"))\\\n                .map(lambda x:(bc.value[str(x[0])],date_to_float(x[1]),time_to_float(x[2]),float(x[3])))\\\n                .map(lambda x: LabeledPoint(x[3], [x[0][0], x[0][1], x[1], x[2]]))\n\ntrain_data, test_data = labeled_data.randomSplit([0.7, 0.3])\n\n# Train the linear regression model\nmodel = LinearRegressionWithSGD.train(train_data, iterations=100, step=0.1)\n\nprint(\"Training complete!\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "AttributeError: 'PipelinedRDD' object has no attribute '_jdf'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# predictions = model.predict(test_data.map(lambda lp: lp.features))\n# print(predictions.collect()[:10])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 70,
			"outputs": [
				{
					"name": "stdout",
					"text": "[-3.8152610095033223e+217, -4.853149398768832e+217, -3.9679862064606803e+217, -4.003230482681609e+217, -4.1148373573812167e+217, -5.146851700609906e+217, -3.196668416385178e+217, -2.223394533524704e+217, -3.519740948410359e+217, -3.742954697809575e+217]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Predict!\na = 58.4274 # latitude\nb = 14.826 # longitude\ndate1 = \"2014-11-04\" # Up to you\n\ny_linreg=[]\npredict_data=[]\nfor time1 in [\"0:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n\"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]:\n    # Your code here\n    predict_data.append([b,a,date_to_float(date1),time_to_float(time1)])\n    \nrdd1 = sc.parallelize(predict_data)\nprint(\"Training data:\")\nprint(rdd1.collect())\npredictions = model.predict(rdd1)\ny_linreg=predictions.collect()\n\nprint(\"Predict result:\")\nprint(y_linreg)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "[[14.826, 58.4274, 84.34065934065934, 0.0], [14.826, 58.4274, 84.34065934065934, 91.66666666666666], [14.826, 58.4274, 84.34065934065934, 83.33333333333334], [14.826, 58.4274, 84.34065934065934, 75.0], [14.826, 58.4274, 84.34065934065934, 66.66666666666666], [14.826, 58.4274, 84.34065934065934, 58.333333333333336], [14.826, 58.4274, 84.34065934065934, 50.0], [14.826, 58.4274, 84.34065934065934, 41.66666666666667], [14.826, 58.4274, 84.34065934065934, 33.33333333333333], [14.826, 58.4274, 84.34065934065934, 25.0], [14.826, 58.4274, 84.34065934065934, 16.666666666666664]]\n[-3.3706059605534134e+217, -5.254079444267832e+217, -5.082854582111976e+217, -4.911629719956119e+217, -4.740404857800263e+217, -4.569179995644407e+217, -4.397955133488551e+217, -4.2267302713326947e+217, -4.055505409176838e+217, -3.884280547020982e+217, -3.7130556848651257e+217]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Neural Network",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}